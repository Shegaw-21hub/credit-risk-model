# Credit Risk Modeling Project

This repository hosts a comprehensive credit risk modeling project, from initial data understanding and preprocessing to model training, evaluation, and experiment tracking using MLflow.

## Credit Scoring Business Understanding

### 1. Basel II Accord and Model Interpretability

The Basel II Accord emphasizes three pillars: minimum capital requirements, supervisory review, and market discipline. For our model:
- **Regulatory Compliance**: The accord requires banks to demonstrate that their risk measurement systems are conceptually sound and empirically validated. This necessitates an interpretable model where decision-making can be clearly explained to regulators.
- **Documentation**: We must thoroughly document our modeling process, including feature selection, proxy variable creation, and validation procedures.
- **Risk Sensitivity**: The model must accurately differentiate risk levels to ensure proper capital allocation according to risk weights.

### 2. Proxy Variable Necessity and Risks

Since we lack direct default data:
- **Necessity**: A proxy based on RFM (Recency, Frequency, Monetary) metrics allows us to approximate credit risk from transactional behavior patterns that correlate with repayment likelihood.
- **Business Risks**:
    - **Misclassification Risk**: The proxy might mislabel some customers, leading to either lost revenue (false positives) or increased defaults (false negatives).
    - **Concept Drift**: Behavioral patterns in e-commerce may not perfectly correlate with credit repayment behavior.
    - **Regulatory Scrutiny**: We must justify our proxy methodology to satisfy compliance requirements.

### 3. Model Complexity Trade-offs

**Simple Models (Logistic Regression with WoE)**:
- *Advantages*: Easily interpretable, compliant with "right to explanation" regulations, simpler to validate and audit.
- *Disadvantages*: May miss complex nonlinear relationships, potentially lower predictive power.

**Complex Models (Gradient Boosting)**:
- *Advantages*: Higher predictive accuracy, can capture intricate feature interactions.
- *Disadvantages*: "Black box" nature raises regulatory concerns, harder to explain decisions to customers.

**Recommended Approach**: Start with interpretable models and only increase complexity if justified by significant performance gains that outweigh regulatory costs.
## Project Structure

The project is organized into logical directories to ensure maintainability and clarity:
```
credit-risk-model/
├── .github/
│   └── workflows/
│       └── ci.yml                # GitHub Actions workflow for CI (from task-6-ci)
├── data/
│   ├── processed/
│   │   ├── eda_plots/            # Directory for EDA visualizations (from task-2-eda)
│   │   │   └── * # Placeholder for EDA plot files
│   │   └── model_ready_data.csv  # Cleaned and processed data (output of data_processing.py)
│   └── raw/
│       └── [raw_data_files]      # Original, raw dataset files
├── models/
│   └── best_model.pkl            # Saved artifact of the best trained model (from task-4-model)
├── notebooks/
│   └── 1.0-eda.ipynb             # Jupyter Notebook for Exploratory Data Analysis (from task-2-eda)
├── src/
│   ├── api/                      # Directory for API source code (from task-5-api)
│   │   ├── main.py
│   │   └── pydantic_models.py
│   ├── data_processing.py        # Script for data cleaning and feature engineering (from task-3-features)
│   └── train.py                  # Script for model training, evaluation, and MLflow tracking (from task-4-model)
├── tests/
│   └── test_data_processing.py   # Unit tests for the data_processing.py script (from task-3-features)
├── .gitignore                    # Specifies intentionally untracked files to ignore (core file)
├── Dockerfile                    # Dockerfile for containerization (from task-5-api)
├── docker-compose.yml            # Docker Compose configuration (from task-5-api)
├── mlruns/                       # MLflow local tracking directory (generated by train.py)
├── README.md                     # Project overview and documentation (core file)
└── requirements.txt              # Python dependencies for the project (core file)
```
## Setup and Running the Project

Follow these steps to set up the project environment and run the model training pipeline:

### Prerequisites

* Python 3.8+
* `pip` (Python package installer)

### 1. Clone the Repository

```bash
git clone [https://github.com/your-username/credit-risk-model.git](https://github.com/your-username/credit-risk-model.git)
cd credit-risk-model
python -m venv venv
# On Windows
.\venv\Scripts\activate
# On macOS/Linux
source venv/bin/activate
### Coding partner

Show thinking
I understand. It seems you're having trouble copying the entire content at once. My apologies for that inconvenience.

I will provide the complete README.md content for you in smaller, more manageable markdown blocks, section by section. Please copy each block and paste them one after the other into your README.md file.

Part 1: Credit Risk Modeling Project (Main Title & Business Understanding)

Markdown

# Credit Risk Modeling Project

This repository hosts a comprehensive credit risk modeling project, from initial data understanding and preprocessing to model training, evaluation, and experiment tracking using MLflow.

## Credit Scoring Business Understanding

### 1. Basel II Accord and Model Interpretability

The Basel II Accord emphasizes three pillars: minimum capital requirements, supervisory review, and market discipline. For our model:
- **Regulatory Compliance**: The accord requires banks to demonstrate that their risk measurement systems are conceptually sound and empirically validated. This necessitates an interpretable model where decision-making can be clearly explained to regulators.
- **Documentation**: We must thoroughly document our modeling process, including feature selection, proxy variable creation, and validation procedures.
- **Risk Sensitivity**: The model must accurately differentiate risk levels to ensure proper capital allocation according to risk weights.

### 2. Proxy Variable Necessity and Risks

Since we lack direct default data:
- **Necessity**: A proxy based on RFM (Recency, Frequency, Monetary) metrics allows us to approximate credit risk from transactional behavior patterns that correlate with repayment likelihood.
- **Business Risks**:
    - **Misclassification Risk**: The proxy might mislabel some customers, leading to either lost revenue (false positives) or increased defaults (false negatives).
    - **Concept Drift**: Behavioral patterns in e-commerce may not perfectly correlate with credit repayment behavior.
    - **Regulatory Scrutiny**: We must justify our proxy methodology to satisfy compliance requirements.

### 3. Model Complexity Trade-offs

**Simple Models (Logistic Regression with WoE)**:
- *Advantages*: Easily interpretable, compliant with "right to explanation" regulations, simpler to validate and audit.
- *Disadvantages*: May miss complex nonlinear relationships, potentially lower predictive power.

**Complex Models (Gradient Boosting)**:
- *Advantages*: Higher predictive accuracy, can capture intricate feature interactions.
- *Disadvantages*: "Black box" nature raises regulatory concerns, harder to explain decisions to customers.

**Recommended Approach**: Start with interpretable models and only increase complexity if justified by significant performance gains that outweigh regulatory costs.
Part 2: Project Structure

Markdown

## Project Structure

The project is organized into logical directories to ensure maintainability and clarity:

credit-risk-model/
├── data/
│   └── processed/
│       └── model_ready_data.csv    # Cleaned and processed data ready for modeling
│   └── raw/
│       └── [raw_data_files]        # Original, raw dataset files
├── models/
│   └── best_model.pkl              # Saved artifact of the best trained model
├── src/
│   ├── 1.0-eda.ipynb               # Jupyter Notebook for Exploratory Data Analysis
│   ├── data_processing.py          # Script for data cleaning and feature engineering
│   ├── test_data_processing.py     # Unit tests for the data_processing.py script
│   └── train.py                    # Script for model training, evaluation, and MLflow tracking
├── mlruns/                         # MLflow local tracking directory
├── .gitignore                      # Specifies intentionally untracked files to ignore
├── README.md                       # Project overview and documentation
└── requirements.txt                # Python dependencies for the project

Part 3: Setup and Running the Project

Markdown

## Setup and Running the Project

Follow these steps to set up the project environment and run the model training pipeline:

### Prerequisites

* Python 3.8+
* `pip` (Python package installer)

### 1. Clone the Repository

```bash
git clone [https://github.com/your-username/credit-risk-model.git](https://github.com/your-username/credit-risk-model.git)
cd credit-risk-model
2. Create and Activate a Virtual Environment
It's highly recommended to use a virtual environment to manage dependencies.

Bash

python -m venv venv
# On Windows
.\venv\Scripts\activate
# On macOS/Linux
source venv/bin/activate
```
3. Install Dependencies
Install all required Python packages from requirements.txt:
```
pip install -r requirements.txt
```
### Data Acquisition (Manual Step - if not already in data/raw)
Ensure your raw data files are placed in the data/raw/ directory according to project specifications.
###  Run the Data Processing Pipeline
```
python src/data_processing.py
```
### Run Data Processing Tests 
To ensure the data processing pipeline is robust:
```
python src/test_data_processing.py
```
###  Train Models and Track Experiments
This script trains various models, performs hyperparameter tuning, evaluates them, and logs all results using MLflow.
```
python src/train.py
```
### View MLflow UI (Experiment Tracking)
After training, you can launch the MLflow UI to inspect the experiments, compare model runs, and manage registered models.
```
mlflow ui
```
### Component Details (Data Exploration & Data Processing)

```markdown
## Component Details

### 1. Data Exploration ( `src/1.0-eda.ipynb` )

* **Purpose:** To gain a deep understanding of the raw dataset, identify data quality issues (missing values, outliers), explore distributions, and uncover potential relationships between features and the target variable (`is_high_risk` proxy).
* **Key Activities:**
    * Univariate and bivariate analysis of key financial and behavioral attributes.
    * Visualization of data distributions and correlations.
    * Identification of patterns that might inform feature engineering strategies.
* **Outcome:** Insights used to guide the development of the data preprocessing and feature engineering steps.

### 2. Data Preprocessing & Feature Engineering ( `src/data_processing.py` )

* **Purpose:** Transforms the raw data into a clean, normalized, and enriched format suitable for machine learning model training.
* **Methodology:**
    * **Handling Missing Values:** Imputation or removal strategies based on data characteristics.
    * **Categorical Encoding:** Techniques like One-Hot Encoding or custom methods for categorical features.
    * **Numerical Scaling:** Standardization or normalization of numerical features to ensure models perform optimally.
    * **Feature Creation (RFM & WoE):** Generates essential features from transactional data, including Recency, Frequency, and Monetary values. Applies Weight of Evidence (WoE) transformation to categorical variables to capture their predictive power for credit risk.
* **Output:** Generates `data/processed/model_ready_data.csv`, the primary input for model training.

### 3. Data Processing Tests ( `src/test_data_processing.py` )

* **Purpose:** Ensures the robustness and correctness of the `data_processing.py` script.
* **Methodology:** Contains unit tests that validate various functions within the data processing pipeline, checking for expected outputs and handling of edge cases. This helps prevent data inconsistencies and pipeline failures.

### 4. Model Training and Experiment Tracking ( `src/train.py` )

* **Purpose:** Develops, evaluates, and tracks various machine learning models for credit risk prediction, ensuring reproducibility and comparability of experiments.
* **Models Trained:**
    * **Logistic Regression:** An interpretable baseline model, often used with WoE transformed features in credit scoring.
    * **RandomForestClassifier:** An ensemble method known for its robustness and ability to capture non-linear relationships.
    * **GradientBoostingClassifier:** Another powerful ensemble method that builds models sequentially to improve predictions.
* **Methodology:**
    * **Data Splitting:** Divides the `model_ready_data.csv` into training and testing sets using stratified sampling to maintain target distribution.
    * **Hyperparameter Tuning:** Employs `GridSearchCV` with 5-fold cross-validation to find optimal hyperparameters for each model, optimizing for `roc_auc` score.
    * **Evaluation Metrics:** Reports Accuracy, Precision, Recall, F1-Score, ROC AUC, and a Confusion Matrix to provide a comprehensive view of model performance.
    * **Model Persistence:** Saves the best performing model locally as `models/best_model.pkl`.
* **MLflow Integration:**
    * Utilizes MLflow for comprehensive experiment tracking, logging:
        * Model hyperparameters.
        * Evaluation metrics (including ROC AUC, F1-Score, etc.).
        * Confusion matrix (as an artifact).
        * Trained model artifacts, complete with signature and input example.
    * The best model is also registered in the MLflow Model Registry as `CreditRiskClassifier` for versioning and deployment readiness.
    ## Results and Model Performance

The training pipeline completed successfully, producing the following high-level performance metrics for the best models on the test set:

* **Logistic Regression:** ROC AUC: 1.0000, F1 Score: 0.9999
* **RandomForestClassifier:** ROC AUC: 1.0000, F1 Score: 1.0000
* **GradientBoostingClassifier:** ROC AUC: 1.0000, F1 Score: 1.0000

**Note on High Performance:** The exceptionally high ROC AUC and F1 scores (approaching 1.0) suggest that the models are performing very well. However, in real-world scenarios, such perfect scores can sometimes indicate **data leakage**. While the current pipeline is robust, future work should include a thorough investigation of the feature engineering process to rule out any inadvertent leakage of target information into features, ensuring the model's true generalization capability.

## Future Work and Improvements

* **Data Leakage Analysis:** Conduct a deeper dive into features to rigorously check for any form of data leakage, especially given the high model performance.
* **Advanced Feature Engineering:** Explore more sophisticated feature interactions or external data sources.
* **Class Imbalance Strategies:** Implement advanced techniques for handling the dataset's class imbalance (e.g., SMOTE, ADASYN, cost-sensitive learning) to potentially improve minority class prediction.
* **Model Deployment:** Develop an API for deploying the `CreditRiskClassifier` for real-time predictions.
* **Monitoring:** Implement monitoring for model drift and data drift in a production environment.
* **User Interface:** Create a simple dashboard or UI to interact with the model and visualize predictions.